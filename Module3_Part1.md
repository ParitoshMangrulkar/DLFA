## Module3 Part1
#### 21 May - Logistic Regression & Multi Class - Rajiv
#### 28 May - Back Propogation, Regularization & Optimization - Rajiv
#### 4 June - Optimization ( Momentum, Adagrad, RMS Prop, Adam ) , Batch Normalization & CNN - Rajiv
#### 11 June - RNN - Shriram
#### 18 June - Attention Network & Deep Learning - Shriram

###### Logistic Regression
###### FeedForward Neural Networks
###### Backpropogation
###### CNN
###### RNN
###### Regularization
###### Dropout
###### batch Normalization
###### Deep Networks
###### Optimization algorithums for training deep networks
###### LSTM
###### Network in Network architectures
###### Attention in Neural Networks

Optimization Algorithm graphics : https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c#:~:text=The%20Momentum%20method%20uses%20the,is%20generally%20the%20best%20choice.


## Test Skills:
### https://www.analyticsvidhya.com/blog/2021/05/artificial-neural-networks-25-questions-to-test-your-skills-on-ann/

### https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-logistic-regression/

###https://learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/
CNN Calculations

### Here are some methods that are proposed to overcome the vanishing gradient problem:
Residual neural networks (ResNets)
Multi-level hierarchy
Long short term memory (LSTM)
Faster hardware
ReLU
Batch normalization
